{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brew install apt-get\n",
    "apt-get update # Update apt-get repository.\n",
    "apt-get openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
    "wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
    "tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
    "pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
    "\n",
    "# Append the directory containing the config module to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'config')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, explode, size, array, sort_array, struct\n",
    "\n",
    "# Append the directory containing the config module to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'config')))\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Friend Recommendations\") \\\n",
    "    .config(\"spark.jars.packages\", \"mysql:mysql-connector-java:8.0.11\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, explode, size, array, sort_array, struct\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load data from CSV\n",
    "df = spark.read.csv(\"User_Friends.csv\", header=True, inferSchema=True).selectExpr(\"cast(user_id as int)\", \"cast(friend_id as int)\")\n",
    "\n",
    "# Create symmetric pairs (bi-directional relationships)\n",
    "friends = df.union(df.select(col(\"friend_id\").alias(\"user_id\"), col(\"user_id\").alias(\"friend_id\")))\n",
    "\n",
    "# Join on user_id to find friends of friends\n",
    "connections = friends.alias(\"f1\").join(friends.alias(\"f2\"), col(\"f1.friend_id\") == col(\"f2.user_id\")) \\\n",
    "    .select(col(\"f1.user_id\"), col(\"f2.friend_id\").alias(\"fof_id\")) \\\n",
    "    .where(col(\"f1.user_id\") != col(\"f2.friend_id\"))\n",
    "\n",
    "# Deduplicate and count mutual friends\n",
    "mutual_friends = connections.groupBy(\"user_id\", \"fof_id\").count()\n",
    "\n",
    "recommendations_struct = mutual_friends.select(\n",
    "    \"user_id\",\n",
    "    struct(col(\"fof_id\"), col(\"count\").alias(\"mutual_friends\")).alias(\"recommendation\")\n",
    ")\n",
    "\n",
    "# Order the DataFrame by user_id and mutual_friends count descending\n",
    "ordered_recommendations = recommendations_struct.orderBy(\"user_id\", col(\"recommendation.mutual_friends\").desc())\n",
    "\n",
    "# Group by user_id and collect recommendations into a list\n",
    "final_recommendations = ordered_recommendations.groupBy(\"user_id\").agg(\n",
    "    collect_list(\"recommendation\").alias(\"recommendations\")\n",
    ")\n",
    "\n",
    "# Show results to verify\n",
    "final_recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert the array of structs to a string for better readability\n",
    "def format_recommendations(recs):\n",
    "    return \", \".join([f\"({x['fof_id']}, {x['mutual_friends']})\" for x in recs])\n",
    "\n",
    "# Register the UDF (User Defined Function)\n",
    "format_udf = udf(format_recommendations, StringType())\n",
    "\n",
    "# Apply UDF to convert array of structs to a formatted string\n",
    "formatted_df = final_recommendations.withColumn(\"recommendations\", format_udf(col(\"recommendations\")))\n",
    "\n",
    "# Select the necessary columns, you might want to keep user_id as integer, it's supported in CSV\n",
    "final_df = formatted_df.select(\"user_id\", \"recommendations\")\n",
    "\n",
    "pandas_df = final_df.toPandas()\n",
    "\n",
    "output_file_path = \"output/friends_rec.csv\"\n",
    "pandas_df.to_csv(output_file_path, index=False) \n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
